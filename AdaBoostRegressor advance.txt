import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from itertools import combinations
import os

# Define the path to the file containing descriptors
os.chdir("C:\\Users\\HP\\Desktop\\ewmachine")  # Update this path to your file location
# Read the descriptors from the file
data = pd.read_csv("data_file.csv")
# Define the target variable (dependent variable)
target = 'Hardness'  

# Define the maximum number of descriptors (features) to consider
n = len(data.columns) - 1  # Exclude the target column

# Get the list of feature names
feature_names = data.columns.drop(target)

# Initialize a dictionary to store results
results = {
    'Number of Descriptors': [],
    'Descriptors': [],
    'Best Hyperparameters': [],
    'R2 Score (CV)': [],
    'RMSE (CV)': []
}

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of weak learners
    'learning_rate': [0.01, 0.1, 0.5, 1.0],  # Learning rate
    'estimator__max_depth': [1, 3, 5]  # Max depth of the base estimator (decision tree)
}

# Loop through the number of descriptors from 1 to n
for num_descriptors in range(1, n + 1):
    # Generate all combinations of descriptors of size 'num_descriptors'
    for descriptor_combination in combinations(feature_names, num_descriptors):
        # Prepare the data
        X = data[list(descriptor_combination)]
        y = data[target]
        
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Initialize the base estimator (decision tree)
        estimator = DecisionTreeRegressor(random_state=42)
        
        # Initialize the AdaBoost model
        model = AdaBoostRegressor(estimator=estimator, random_state=42)
        
        # Perform Grid Search for hyperparameter tuning
        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        
        # Get the best model and hyperparameters
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
        
        # Perform cross-validation on the best model
        cv_r2_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')
        cv_rmse_scores = np.sqrt(-cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))
        
        # Calculate average cross-validation scores
        avg_r2 = np.mean(cv_r2_scores)
        avg_rmse = np.mean(cv_rmse_scores)
        
        # Store the results
        results['Number of Descriptors'].append(num_descriptors)
        results['Descriptors'].append(', '.join(descriptor_combination))
        results['Best Hyperparameters'].append(str(best_params))
        results['R2 Score (CV)'].append(avg_r2)
        results['RMSE (CV)'].append(avg_rmse)

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Save the results to a CSV file in the same folder
output_file = 'adaboost_regression_advanced_results.csv'
results_df.to_csv(output_file, index=False)

print(f"Results saved to {output_file}")